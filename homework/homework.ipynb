{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa: E501\n",
    "#\n",
    "# En este dataset se desea pronosticar el default (pago) del cliente el próximo\n",
    "# mes a partir de 23 variables explicativas.\n",
    "#\n",
    "#   LIMIT_BAL: Monto del credito otorgado. Incluye el credito individual y el\n",
    "#              credito familiar (suplementario).\n",
    "#         SEX: Genero (1=male; 2=female).\n",
    "#   EDUCATION: Educacion (0=N/A; 1=graduate school; 2=university; 3=high school; 4=others).\n",
    "#    MARRIAGE: Estado civil (0=N/A; 1=married; 2=single; 3=others).\n",
    "#         AGE: Edad (years).\n",
    "#       PAY_0: Historia de pagos pasados. Estado del pago en septiembre, 2005.\n",
    "#       PAY_2: Historia de pagos pasados. Estado del pago en agosto, 2005.\n",
    "#       PAY_3: Historia de pagos pasados. Estado del pago en julio, 2005.\n",
    "#       PAY_4: Historia de pagos pasados. Estado del pago en junio, 2005.\n",
    "#       PAY_5: Historia de pagos pasados. Estado del pago en mayo, 2005.\n",
    "#       PAY_6: Historia de pagos pasados. Estado del pago en abril, 2005.\n",
    "#   BILL_AMT1: Historia de pagos pasados. Monto a pagar en septiembre, 2005.\n",
    "#   BILL_AMT2: Historia de pagos pasados. Monto a pagar en agosto, 2005.\n",
    "#   BILL_AMT3: Historia de pagos pasados. Monto a pagar en julio, 2005.\n",
    "#   BILL_AMT4: Historia de pagos pasados. Monto a pagar en junio, 2005.\n",
    "#   BILL_AMT5: Historia de pagos pasados. Monto a pagar en mayo, 2005.\n",
    "#   BILL_AMT6: Historia de pagos pasados. Monto a pagar en abril, 2005.\n",
    "#    PAY_AMT1: Historia de pagos pasados. Monto pagado en septiembre, 2005.\n",
    "#    PAY_AMT2: Historia de pagos pasados. Monto pagado en agosto, 2005.\n",
    "#    PAY_AMT3: Historia de pagos pasados. Monto pagado en julio, 2005.\n",
    "#    PAY_AMT4: Historia de pagos pasados. Monto pagado en junio, 2005.\n",
    "#    PAY_AMT5: Historia de pagos pasados. Monto pagado en mayo, 2005.\n",
    "#    PAY_AMT6: Historia de pagos pasados. Monto pagado en abril, 2005.\n",
    "#\n",
    "# La variable \"default payment next month\" corresponde a la variable objetivo.\n",
    "#\n",
    "# El dataset ya se encuentra dividido en conjuntos de entrenamiento y prueba\n",
    "# en la carpeta \"files/input/\".\n",
    "#\n",
    "# Los pasos que debe seguir para la construcción de un modelo de\n",
    "# clasificación están descritos a continuación.\n",
    "#\n",
    "#\n",
    "# Paso 1.\n",
    "# Realice la limpieza de los datasets:\n",
    "# - Renombre la columna \"default payment next month\" a \"default\".\n",
    "# - Remueva la columna \"ID\".\n",
    "# - Elimine los registros con informacion no disponible.\n",
    "# - Para la columna EDUCATION, valores > 4 indican niveles superiores\n",
    "#   de educación, agrupe estos valores en la categoría \"others\".\n",
    "# - Renombre la columna \"default payment next month\" a \"default\"\n",
    "# - Remueva la columna \"ID\".\n",
    "#\n",
    "#\n",
    "# Paso 2.\n",
    "# Divida los datasets en x_train, y_train, x_test, y_test.\n",
    "#\n",
    "#\n",
    "# Paso 3.\n",
    "# Cree un pipeline para el modelo de clasificación. Este pipeline debe\n",
    "# contener las siguientes capas:\n",
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "# - Descompone la matriz de entrada usando PCA. El PCA usa todas las componentes.\n",
    "# - Estandariza la matriz de entrada.\n",
    "# - Selecciona las K columnas mas relevantes de la matrix de entrada.\n",
    "# - Ajusta una maquina de vectores de soporte (svm).\n",
    "#\n",
    "#\n",
    "# Paso 4.\n",
    "# Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "# Use 10 splits para la validación cruzada. Use la función de precision\n",
    "# balanceada para medir la precisión del modelo.\n",
    "#\n",
    "#\n",
    "# Paso 5.\n",
    "# Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "# Recuerde que es posible guardar el modelo comprimido usanzo la libreria gzip.\n",
    "#\n",
    "#\n",
    "# Paso 6.\n",
    "# Calcule las metricas de precision, precision balanceada, recall,\n",
    "# y f1-score para los conjuntos de entrenamiento y prueba.\n",
    "# Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# Este diccionario tiene un campo para indicar si es el conjunto\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'dataset': 'train', 'precision': 0.8, 'balanced_accuracy': 0.7, 'recall': 0.9, 'f1_score': 0.85}\n",
    "# {'dataset': 'test', 'precision': 0.7, 'balanced_accuracy': 0.6, 'recall': 0.8, 'f1_score': 0.75}\n",
    "#\n",
    "#\n",
    "# Paso 7.\n",
    "# Calcule las matrices de confusion para los conjuntos de entrenamiento y\n",
    "# prueba. Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'type': 'cm_matrix', 'dataset': 'train', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 666}, 'true_1': {\"predicted_0\": 3333, \"predicted_1\": 1444}}\n",
    "# {'type': 'cm_matrix', 'dataset': 'test', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 650}, 'true_1': {\"predicted_0\": 2490, \"predicted_1\": 1420}}\n",
    "#\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import precision_score, balanced_accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(data_df):\n",
    "    \n",
    "    df=data_df.copy()\n",
    "    df=df.rename(columns={'default payment next month': 'default'})\n",
    "    df=df.drop(columns='ID')\n",
    "    df['EDUCATION'] = df['EDUCATION'].replace(0, np.nan)\n",
    "    df['MARRIAGE'] = df['MARRIAGE'].replace(0, np.nan)\n",
    "    df=df.dropna()\n",
    "    df.loc[df['EDUCATION'] > 4, 'EDUCATION'] = 4\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Paso 2.\n",
    "# Divida los datasets en x_train, y_train, x_test, y_test.\n",
    "#\n",
    "\n",
    "def get_features_target(data, target_column):\n",
    "    x = data.drop(columns=target_column)\n",
    "    y = data[target_column]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Paso 3.\n",
    "# Cree un pipeline para el modelo de clasificación. Este pipeline debe\n",
    "# contener las siguientes capas:\n",
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "# - Descompone la matriz de entrada usando PCA. El PCA usa todas las componentes.\n",
    "# - Estandariza la matriz de entrada.\n",
    "# - Selecciona las K columnas mas relevantes de la matrix de entrada.\n",
    "# - Ajusta una maquina de vectores de soporte (svm).\n",
    "#\n",
    "\n",
    "def create_pipeline(df):\n",
    "    categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "    numerical_features = [col for col in df.columns if col not in categorical_features]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "            ('num', StandardScaler(), numerical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('pca', PCA()),\n",
    "            ('select_k_best', SelectKBest(f_classif)),\n",
    "            ('model', SVC())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# Paso 4.\n",
    "# Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "# Use 10 splits para la validación cruzada. Use la función de precision\n",
    "# balanceada para medir la precisión del modelo.\n",
    "#\n",
    "\n",
    "def optimize_hyperparameters(pipeline, x_train, y_train):\n",
    "    param_grid = {\n",
    "        'pca__n_components': [21],\n",
    "        'select_k_best__k': [12],\n",
    "        'model__C': [0.8],\n",
    "        'model__kernel': ['rbf'],\n",
    "        'model__gamma': [0.1],\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='balanced_accuracy', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "# Paso 5.\n",
    "# Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "# Recuerde que es posible guardar el modelo comprimido usanzo la libreria gzip.\n",
    "#\n",
    "\n",
    "def save_model(model):\n",
    "    if not os.path.exists('../files/models'):\n",
    "        os.makedirs('../files/models')\n",
    "    with gzip.open('../files/models/model.pkl.gz', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "\n",
    "#\n",
    "# Paso 6.\n",
    "# Calcule las metricas de precision, precision balanceada, recall,\n",
    "# y f1-score para los conjuntos de entrenamiento y prueba.\n",
    "# Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# Este diccionario tiene un campo para indicar si es el conjunto\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'dataset': 'train', 'precision': 0.8, 'balanced_accuracy': 0.7, 'recall': 0.9, 'f1_score': 0.85}\n",
    "# {'dataset': 'test', 'precision': 0.7, 'balanced_accuracy': 0.6, 'recall': 0.8, 'f1_score': 0.75}\n",
    "#\n",
    "\n",
    "def calculate_metrics(model, x_train, y_train, x_test, y_test):\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    metrics_train = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'train',\n",
    "        'precision': float(round(precision_score(y_train, y_train_pred),3)),\n",
    "        'balanced_accuracy': float(round(balanced_accuracy_score(y_train, y_train_pred),3)),\n",
    "        'recall': float(round(recall_score(y_train, y_train_pred),3)),\n",
    "        'f1_score': float(round(f1_score(y_train, y_train_pred),3))\n",
    "    }\n",
    "\n",
    "    metrics_test = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'test',\n",
    "        'precision': float(round(precision_score(y_test, y_test_pred),3)),\n",
    "        'balanced_accuracy': float(round(balanced_accuracy_score(y_test, y_test_pred),3)),\n",
    "        'recall': float(round(recall_score(y_test, y_test_pred),3)),\n",
    "        'f1_score': float(round(f1_score(y_test, y_test_pred),3))\n",
    "    }\n",
    "\n",
    "    return metrics_train, metrics_test\n",
    "\n",
    "\n",
    "#\n",
    "# Paso 7.\n",
    "# Calcule las matrices de confusion para los conjuntos de entrenamiento y\n",
    "# prueba. Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'type': 'cm_matrix', 'dataset': 'train', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 666}, 'true_1': {\"predicted_0\": 3333, \"predicted_1\": 1444}}\n",
    "# {'type': 'cm_matrix', 'dataset': 'test', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 650}, 'true_1': {\"predicted_0\": 2490, \"predicted_1\": 1420}}\n",
    "#\n",
    "\n",
    "def calculate_confusion_matrix(model, x_train, y_train, x_test, y_test):\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    cm_matrix_train = {\n",
    "        'type': 'cm_matrix',\n",
    "        'dataset': 'train',\n",
    "        'true_0': {\"predicted_0\": int(cm_train[0, 0]), \"predicted_1\": int(cm_train[0, 1])},\n",
    "        'true_1': {\"predicted_0\": int(cm_train[1, 0]), \"predicted_1\": int(cm_train[1, 1])}\n",
    "    }\n",
    "\n",
    "    cm_matrix_test = {\n",
    "        'type': 'cm_matrix',\n",
    "        'dataset': 'test',\n",
    "        'true_0': {\"predicted_0\": int(cm_test[0, 0]), \"predicted_1\": int(cm_test[0, 1])},\n",
    "        'true_1': {\"predicted_0\": int(cm_test[1, 0]), \"predicted_1\": int(cm_test[1, 1])}\n",
    "    }\n",
    "\n",
    "    return cm_matrix_train, cm_matrix_test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    train_data_zip = '../files/input/train_data.csv.zip'\n",
    "    test_data_zip = '../files/input/test_data.csv.zip'\n",
    "\n",
    "    train_data=pd.read_csv(\n",
    "        train_data_zip,\n",
    "        index_col=False,\n",
    "        compression='zip')\n",
    "\n",
    "    test_data=pd.read_csv(\n",
    "        test_data_zip,\n",
    "        index_col=False,\n",
    "        compression='zip')\n",
    "\n",
    "    train_data=clean_data(train_data)\n",
    "    test_data=clean_data(test_data)\n",
    "\n",
    "    x_train, y_train = get_features_target(train_data, 'default')\n",
    "    x_test, y_test = get_features_target(test_data, 'default')\n",
    "\n",
    "    pipeline = create_pipeline(x_train)\n",
    "\n",
    "    start = time.time()\n",
    "    model = optimize_hyperparameters(pipeline, x_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    save_model(model)\n",
    "\n",
    "    metrics_train, metrics_test = calculate_metrics(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    cm_matrix_train, cm_matrix_test = calculate_confusion_matrix(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "    if not os.path.exists('../files/output'):\n",
    "        os.makedirs('../files/output')\n",
    "\n",
    "    metrics = [metrics_train, metrics_test, cm_matrix_train, cm_matrix_test]\n",
    "    pd.DataFrame(metrics).to_json('../files/output/metrics.json', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
